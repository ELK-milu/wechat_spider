#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·çˆ¬è™« - åŸºäºç”¨æˆ·ä¿®æ­£åçš„link_spideré‡æ–°åˆå¹¶
å®Œå…¨ä¿æŒç”¨æˆ·åŸå§‹ä»£ç çš„é€»è¾‘å’Œé…ç½®
"""

import os
import requests
import datetime
import json
import time
import math
import random
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

class WeChatSpider:
    def __init__(self):
        # ===ã€é…ç½®é¡¹ - åŸºäºç”¨æˆ·ä¿®æ­£åçš„ä»£ç ã€‘===
        self.FAKEIDS = {
            "ç²¤æ”¿ä¼šè®¡": "MzAxNTM0NzU1Ng==",
            "ç‘å¹¸å’–å•¡": "MzUxNDQ2OTc2MQ==",
            "å¨ç§‘å…ˆè¡Œ": "MzA5MDAyODcwMQ=="
        }
        
        # ä½¿ç”¨ç”¨æˆ·ä¿®æ­£åçš„é…ç½®
        self.TOKEN = "1501983514"
        self.COOKIE = "appmsglist_action_3191926402=card; pgv_pvid=5423298048; ptcz=45db8b0b704ee1d4452bb9fcb496a4d45efe0c6707cf9f468597a4b744e8c778; o2_uin=1072864748; RK=1CfFfUVXZZ; _qimei_q36=; _qimei_h38=19c544adf6ddaab6cd866d6a0200000761861c; eas_sid=F1X7y4Q8Z7l1f0C9L550Q428o1; ua_id=sRdOzkZCOt8LUohMAAAAAA0bTaYSoAcTca2v74L7v8c=; wxuin=52040870730641; mm_lang=zh_CN; xid=5b5f9f5a00b9341beab9bae9cbee9d04; data_bizuin=3191926402; bizuin=3191926402; data_ticket=WeMKPP765jjz6S8qGYRHtOeCIn+jg8+FmboeG228aP1OHxiJj3sWFD6lAk0VC7+i; rand_info=CAESIBOyMZzjNR36/+TFGg17gF2NHS/nKeQsuXJ/86AL1322; slave_bizuin=3191926402; slave_user=gh_8297f1f670b7; slave_sid=ZXhqTFpTOHRpSFVZY2ZTSUpoMzFvZ3N3VnV3cUN3aGt4MGg2ZTR4V25GX19JVVVpeTdiOVFianJSS1RxRWNpUmdJS1lOMnM4Q3BoYXpaR1FmWlFZd2I0alhheXpVR2ZoQjByNDBfczIyRDNUOWlsbWdvVzlBRXZ6UTNWMWc1SWxnaUdQaHRJbzA1bk9pTUNn; _clck=3191926402|1|fxh|0; _clsk=ztwm2m|1752111910751|7|1|mp.weixin.qq.com/weheat-agent/payload/record"
        
        # ç”¨æˆ·çš„User-Agentåˆ—è¡¨
        self.USER_AGENT_LIST = [
            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 115Browser/6.0.3',
            'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',
            'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',
            'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11',
            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0',
            'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',
            "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.75 Mobile Safari/537.36",
        ]
        
        # åˆ›å»ºæ•°æ®æ–‡ä»¶å¤¹ç»“æ„
        os.makedirs("./data", exist_ok=True)
        for account_name in self.FAKEIDS.keys():
            os.makedirs(f"./data/{account_name}", exist_ok=True)
        os.makedirs("./data/mixed", exist_ok=True)
        
        # å­˜å‚¨æ–‡ç« æ•°æ®
        self.articles = []
        self.driver = None
    
    def fetch_article_links(self, begin=0, count=5, account_name=None):
        """è·å–æ–‡ç« é“¾æ¥åˆ—è¡¨ - ä½¿ç”¨ç”¨æˆ·ä¿®æ­£åçš„APIå’Œå‚æ•°"""
        # ç¡®å®šä½¿ç”¨å“ªä¸ªå…¬ä¼—å·
        if account_name and account_name in self.FAKEIDS:
            target_fakeid = self.FAKEIDS[account_name]
            target_name = account_name
        else:
            target_name = list(self.FAKEIDS.keys())[0]
            target_fakeid = self.FAKEIDS[target_name]
        
        # ä½¿ç”¨ç”¨æˆ·ä¿®æ­£åçš„URLå’Œå‚æ•°ç»“æ„
        url = "https://mp.weixin.qq.com/cgi-bin/appmsg"
        
        data = {
            "token": self.TOKEN,
            "lang": "zh_CN",
            "f": "json",
            "ajax": "1",
            "action": "list_ex",
            "begin": str(begin),
            "count": str(count),
            "query": "",
            "fakeid": target_fakeid,
            "type": "9",
        }
        
        # éšæœºé€‰æ‹©User-Agent
        user_agent = random.choice(self.USER_AGENT_LIST)
        headers = {
            "Cookie": self.COOKIE,
            "User-Agent": user_agent,
        }
        
        try:
            print(f"ğŸ“¡ è¯·æ±‚æ–‡ç« é“¾æ¥: {target_name}")
            response = requests.get(url, headers=headers, params=data)
            
            if response.status_code != 200:
                print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
            
            content_json = response.json()
            
            # æ£€æŸ¥è¿”å›ç»“æœ
            if "app_msg_list" not in content_json:
                print(f"è¿”å›æ•°æ®å¼‚å¸¸: {content_json}")
                return []
            
            results = []
            for item in content_json["app_msg_list"]:
                title = item.get("title", "")
                link = item.get("link", "")
                create_time = item.get("create_time", 0)
                
                # è½¬æ¢æ—¶é—´æ ¼å¼
                t = time.localtime(create_time)
                pub_time = time.strftime("%Y-%m-%d %H:%M:%S", t)
                
                results.append({
                    "title": title,
                    "url": link,
                    "pub_time": pub_time,
                    "account_name": target_name
                })
                
            print(f"âœ… è·å–åˆ° {len(results)} ç¯‡æ–‡ç« é“¾æ¥")
            return results
            
        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def fetch_article_content_requests(self, article):
        """ä½¿ç”¨requestsè·å–æ–‡ç« å†…å®¹"""
        try:
            print(f"ğŸ“„ æŠ“å–å†…å®¹: {article['title']}")
            
            # ä½¿ç”¨éšæœºUser-Agent
            user_agent = random.choice(self.USER_AGENT_LIST)
            headers = {"User-Agent": user_agent}
            
            res = requests.get(article["url"], headers=headers, timeout=15)
            
            if res.status_code != 200:
                print(f"âš ï¸ å†…å®¹è¯·æ±‚å¤±è´¥: {res.status_code}")
                return None
                
            soup = BeautifulSoup(res.text, "html.parser")
            
            # è·å–æ–‡ç« å†…å®¹
            content_elem = soup.select_one("div.rich_media_content")
            if not content_elem:
                content_elem = soup.select_one("#js_content")
            content = content_elem.get_text(separator="\n", strip=True) if content_elem else ""
            
            # è·å–æ–‡ç« æ ‡é¢˜
            title_elem = soup.select_one("h1.rich_media_title")
            title = title_elem.get_text(strip=True) if title_elem else article["title"]
            
            # è·å–ä½œè€…ä¿¡æ¯
            author_elem = soup.select_one("span.rich_media_meta_text")
            author = author_elem.get_text(strip=True) if author_elem else article.get("account_name", "æœªçŸ¥å…¬ä¼—å·")
            
            # è·å–å›¾ç‰‡é“¾æ¥
            images = []
            if content_elem:
                img_elements = content_elem.find_all('img')
                for img in img_elements:
                    src = img.get('data-src') or img.get('src')
                    if src and src.startswith('http'):
                        images.append(src)
            
            return {
                "title": title,
                "author": author,
                "account_name": article.get("account_name", "æœªçŸ¥å…¬ä¼—å·"),
                "pub_time": article["pub_time"],
                "url": article["url"],
                "content": content,
                "images": images,
                "word_count": len(content),
                "crawl_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            print(f"âŒ è·å–å†…å®¹å¤±è´¥: {e}")
            return None
    
    def setup_selenium_driver(self):
        """è®¾ç½®Selenium WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--lang=zh-CN")
            
            # å°è¯•ä½¿ç”¨å…·ä½“çš„ChromeDriverè·¯å¾„
            chrome_driver_path = r"D:\å„ç§installer\chormDriver\chromedriver-win64\chromedriver-win64\chromedriver.exe"
            
            if os.path.exists(chrome_driver_path):
                print(f"ä½¿ç”¨æœ¬åœ°ChromeDriver: {chrome_driver_path}")
                service = Service(executable_path=chrome_driver_path)
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            else:
                print("æœ¬åœ°ChromeDriverä¸å­˜åœ¨ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½...")
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            print("âœ… Selenium WebDriver åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ Selenium WebDriver åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def fetch_article_content_selenium(self, article):
        """ä½¿ç”¨Seleniumè·å–æ–‡ç« å†…å®¹"""
        try:
            if not self.driver:
                if not self.setup_selenium_driver():
                    return None
            
            self.driver.get(article["url"])
            time.sleep(2)  # é˜²æ­¢åŠ è½½ä¸å®Œå…¨
            
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            div = soup.find("div", class_="rich_media_content")
            content = div.get_text(separator="\n", strip=True) if div else ""
            
            # è·å–æ ‡é¢˜
            title_elem = soup.select_one("h1.rich_media_title")
            title = title_elem.get_text(strip=True) if title_elem else article["title"]
            
            # è·å–ä½œè€…
            author_elem = soup.select_one("span.rich_media_meta_text")
            author = author_elem.get_text(strip=True) if author_elem else article.get("account_name", "æœªçŸ¥å…¬ä¼—å·")
            
            # è·å–å›¾ç‰‡
            images = []
            if div:
                img_elements = div.find_all('img')
                for img in img_elements:
                    src = img.get('data-src') or img.get('src')
                    if src and src.startswith('http'):
                        images.append(src)
            
            return {
                "title": title,
                "author": author,
                "account_name": article.get("account_name", "æœªçŸ¥å…¬ä¼—å·"),
                "pub_time": article["pub_time"],
                "url": article["url"],
                "content": content,
                "images": images,
                "word_count": len(content),
                "crawl_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            print(f"âŒ Seleniumè·å–å†…å®¹å¤±è´¥: {e}")
            return None
    
    def crawl_latest_article(self, use_selenium=False, account_name=None):
        """çˆ¬å–æœ€æ–°çš„ä¸€ç¯‡æ–‡ç« """
        target_account = account_name or list(self.FAKEIDS.keys())[0]
        print(f"ğŸš€ å¼€å§‹çˆ¬å– {target_account} æœ€æ–°æ–‡ç« ...")
        
        # è·å–æ–‡ç« é“¾æ¥
        links = self.fetch_article_links(begin=0, count=1, account_name=target_account)
        
        if not links:
            print("âŒ æœªè·å–åˆ°æ–‡ç« é“¾æ¥")
            return None
        
        latest_article = links[0]
        print(f"ğŸ“– æ‰¾åˆ°æœ€æ–°æ–‡ç« : {latest_article['title']}")
        
        # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«å°
        time.sleep(random.randint(2, 5))
        
        # è·å–æ–‡ç« è¯¦ç»†å†…å®¹
        if use_selenium:
            detail = self.fetch_article_content_selenium(latest_article)
            if detail is None:
                print("âš ï¸ Seleniumæ–¹æ³•å¤±è´¥ï¼Œå°è¯•requestsæ–¹æ³•...")
                detail = self.fetch_article_content_requests(latest_article)
        else:
            detail = self.fetch_article_content_requests(latest_article)
        
        if detail:
            self.articles = [detail]
            print(f"âœ… æˆåŠŸçˆ¬å–æ–‡ç« : {detail['title']}")
            print(f"ğŸ“± å…¬ä¼—å·: {detail['account_name']}")
            print(f"ğŸ“ æ–‡ç« å­—æ•°: {detail['word_count']} å­—")
            
            # ä¿å­˜æ–‡ä»¶
            self.save_to_json(account_name=target_account)
            self.save_to_excel(account_name=target_account)
            
            return detail
        else:
            print("âŒ æ–‡ç« å†…å®¹è·å–å¤±è´¥")
            return None
    
    def crawl_recent_articles(self, days_back=30, max_articles=10, use_selenium=False, account_name=None):
        """çˆ¬å–æœ€è¿‘ä¸€æ®µæ—¶é—´çš„æ–‡ç« """
        target_account = account_name or list(self.FAKEIDS.keys())[0]
        print(f"ğŸš€ å¼€å§‹çˆ¬å– {target_account} æœ€è¿‘{days_back}å¤©çš„æ–‡ç« ï¼Œæœ€å¤š{max_articles}ç¯‡...")
        
        all_articles = []
        now = datetime.datetime.now()
        cutoff_date = now - datetime.timedelta(days=days_back)
        
        page = 0
        total_fetched = 0
        
        while total_fetched < max_articles:
            print(f"ğŸ“‹ æ­£åœ¨è·å–ç¬¬{page+1}é¡µæ–‡ç« é“¾æ¥...")
            links = self.fetch_article_links(begin=page*5, count=5, account_name=target_account)
            
            if not links:
                print("ğŸ“„ æ²¡æœ‰æ›´å¤šæ–‡ç« äº†")
                break
            
            for article in links:
                if total_fetched >= max_articles:
                    break
                
                try:
                    # æ£€æŸ¥æ–‡ç« æ—¥æœŸ
                    pub_dt = datetime.datetime.strptime(article["pub_time"], "%Y-%m-%d %H:%M:%S")
                    if pub_dt < cutoff_date:
                        print(f"â° æ–‡ç«  '{article['title']}' è¶…å‡ºæ—¶é—´èŒƒå›´ï¼Œè·³è¿‡")
                        continue
                    
                    # è·å–æ–‡ç« è¯¦ç»†å†…å®¹
                    if use_selenium:
                        detail = self.fetch_article_content_selenium(article)
                        if detail is None:
                            detail = self.fetch_article_content_requests(article)
                    else:
                        detail = self.fetch_article_content_requests(article)
                    
                    if detail:
                        all_articles.append(detail)
                        total_fetched += 1
                        print(f"âœ… å·²å®Œæˆ {total_fetched}/{max_articles}")
                        
                        # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                        time.sleep(random.randint(15, 25))
                    else:
                        print(f"âŒ æ–‡ç« å†…å®¹è·å–å¤±è´¥: {article['title']}")
                        
                except Exception as e:
                    print(f"âŒ å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                    continue
            
            page += 1
            time.sleep(random.randint(2, 5))  # é¡µé¢é—´å»¶è¿Ÿ
        
        self.articles = all_articles
        print(f"ğŸ‰ å…±æˆåŠŸçˆ¬å– {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles
    
    def crawl_all_accounts_latest(self, use_selenium=False):
        """éå†æ‰€æœ‰å…¬ä¼—å·ï¼Œçˆ¬å–æ¯ä¸ªå…¬ä¼—å·çš„æœ€æ–°æ–‡ç« """
        print(f"ğŸš€ å¼€å§‹éå†æ‰€æœ‰å…¬ä¼—å·çˆ¬å–æœ€æ–°æ–‡ç« ...")
        print(f"ğŸ“‹ å°†çˆ¬å–çš„å…¬ä¼—å·: {list(self.FAKEIDS.keys())}")
        
        all_articles = []
        
        for account_name in self.FAKEIDS.keys():
            print(f"\n{'='*50}")
            print(f"ğŸ“± æ­£åœ¨å¤„ç†å…¬ä¼—å·: {account_name}")
            
            try:
                latest = self.crawl_latest_article(use_selenium=use_selenium, account_name=account_name)
                if latest:
                    all_articles.append(latest)
                    print(f"âœ… {account_name} æœ€æ–°æ–‡ç« è·å–æˆåŠŸ")
                else:
                    print(f"âŒ {account_name} æœ€æ–°æ–‡ç« è·å–å¤±è´¥")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°ï¼ˆä½¿ç”¨ç”¨æˆ·åŸå§‹ä»£ç ä¸­çš„å»¶è¿Ÿç­–ç•¥ï¼‰
                print("â³ ç­‰å¾…åå¤„ç†ä¸‹ä¸€ä¸ªå…¬ä¼—å·...")
                time.sleep(random.randint(15, 25))
                
            except Exception as e:
                print(f"âŒ å¤„ç†å…¬ä¼—å· {account_name} æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿å­˜æ··åˆæ•°æ®
        if all_articles:
            self.articles = all_articles
            print(f"\nğŸ‰ éå†å®Œæˆï¼å…±æˆåŠŸçˆ¬å– {len(all_articles)} ä¸ªå…¬ä¼—å·çš„æœ€æ–°æ–‡ç« ")
            self.save_to_json()  # ä¿å­˜åˆ°mixedæ–‡ä»¶å¤¹
            self.save_to_excel()
        
        return all_articles
    
    def generate_filename(self, base_name, extension, account_name=None):
        """ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if account_name and account_name in self.FAKEIDS:
            folder = f"./data/{account_name}"
            filename = f"{base_name}_{timestamp}.{extension}"
        else:
            folder = "./data/mixed"
            filename = f"{base_name}_{timestamp}.{extension}"
        
        return os.path.join(folder, filename)
    
    def save_to_json(self, account_name=None):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not self.articles:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        base_name = "latest_article" if len(self.articles) == 1 else "recent_articles"
        filename = self.generate_filename(base_name, "json", account_name)
        
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(self.articles, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ JSONæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def save_to_excel(self, account_name=None):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        if not self.articles:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        base_name = "latest_article" if len(self.articles) == 1 else "recent_articles"
        filename = self.generate_filename(base_name, "xlsx", account_name)
        
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        try:
            df_data = []
            for article in self.articles:
                df_data.append({
                    'å…¬ä¼—å·': article.get('account_name', 'æœªçŸ¥å…¬ä¼—å·'),
                    'æ ‡é¢˜': article['title'],
                    'ä½œè€…': article['author'],
                    'å‘å¸ƒæ—¶é—´': article['pub_time'],
                    'æ–‡ç« é“¾æ¥': article['url'],
                    'å­—æ•°': article['word_count'],
                    'å›¾ç‰‡æ•°é‡': len(article['images']),
                    'å†…å®¹æ‘˜è¦': article['content'][:200] + '...' if len(article['content']) > 200 else article['content'],
                    'çˆ¬å–æ—¶é—´': article['crawl_time']
                })
            
            df = pd.DataFrame(df_data)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"ğŸ“Š Excelæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def save_to_csv(self, account_name=None):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆå…¼å®¹ç”¨æˆ·åŸå§‹ä»£ç çš„ä¿å­˜æ ¼å¼ï¼‰"""
        if not self.articles:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        base_name = "articles_list"
        filename = self.generate_filename(base_name, "csv", account_name)
        
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        try:
            df_data = []
            for article in self.articles:
                df_data.append({
                    'title': article['title'],
                    'link': article['url'],
                    'create_time': article['pub_time']
                })
            
            df = pd.DataFrame(df_data)
            df.to_csv(filename, mode='a', encoding='utf-8', index=False)
            print(f"ğŸ“„ CSVæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def close_driver(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            try:
                self.driver.quit()
                print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
            except:
                pass

def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    spider = WeChatSpider()
    
    try:
        print("ğŸ¯ åŸºäºç”¨æˆ·ä¿®æ­£ä»£ç çš„å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«")
        print(f"ğŸ“‹ æ”¯æŒçš„å…¬ä¼—å·: {list(spider.FAKEIDS.keys())}")
        
        # æµ‹è¯•å•ä¸ªå…¬ä¼—å·
        print("\n" + "="*50)
        print("æµ‹è¯•: çˆ¬å–å¨ç§‘å…ˆè¡Œæœ€æ–°æ–‡ç« ")
        print("="*50)
        
        latest = spider.crawl_latest_article(use_selenium=False, account_name="å¨ç§‘å…ˆè¡Œ")
        if latest:
            print(f"âœ… æµ‹è¯•æˆåŠŸï¼")
            print(f"ğŸ“– æ ‡é¢˜: {latest['title']}")
            print(f"ğŸ“ å­—æ•°: {latest['word_count']}")
        
        # æµ‹è¯•æ‰€æœ‰å…¬ä¼—å·
        spider.articles = []  # æ¸…ç©ºæ•°æ®
        
        print("\n" + "="*50)
        print("æµ‹è¯•: çˆ¬å–æ‰€æœ‰å…¬ä¼—å·æœ€æ–°æ–‡ç« ")
        print("="*50)
        
        all_latest = spider.crawl_all_accounts_latest(use_selenium=False)
        if all_latest:
            print(f"âœ… æ‰€æœ‰å…¬ä¼—å·æµ‹è¯•æˆåŠŸï¼Œå…±è·å– {len(all_latest)} ç¯‡æ–‡ç« ")
            for article in all_latest:
                print(f"ğŸ“± {article['account_name']}: {article['title']} ({article['word_count']} å­—)")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        spider.close_driver()

if __name__ == "__main__":
    main() 