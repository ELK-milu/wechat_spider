#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·çˆ¬è™« - åŸºäºç”¨æˆ·ä¿®æ­£åçš„link_spideré‡æ–°åˆå¹¶
å®Œå…¨ä¿æŒç”¨æˆ·åŸå§‹ä»£ç çš„é€»è¾‘å’Œé…ç½®
"""

import os
import requests
import datetime
import json
import time
import math
import random
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

from article_2_md import extract_wechat_article_to_markdown, check_url_valid


class WeChatSpider:
    def __init__(self):
        # ===ã€é…ç½®é¡¹ - åŸºäºç”¨æˆ·ä¿®æ­£åçš„ä»£ç ã€‘===
        self.FAKEIDS = {
            "èšå…‰ç§‘æŠ€": "MzA3MzEwOTAxOQ==",
        }
        self.chrome_driver_path = r"D:\å„ç§installer\chormDriver\chromedriver-win64\chromedriver-win64\chromedriver.exe"
        # ä½¿ç”¨ç”¨æˆ·ä¿®æ­£åçš„é…ç½®
        self.TOKEN = "516943987"
        self.COOKIE = "pgv_pvid=8898906321; RK=ScNFh3Rq8V; ptcz=22590a12a8e18cc94980e9eb09c767ed7b6ccf56c9207baf45a206f07c77a450; pac_uid=0_ccQAAksry1Ftm; _qimei_uuid42=192130a1c22100a53345d4972209f34e387a197eaf; _qimei_fingerprint=041da21d25c5068368e56d1e1ad58716; _qimei_h38=8f95fb0e3345d4972209f34e0200000e719213; suid=user_0_ccQAAksry1Ftm; qq_domain_video_guid_verify=82e6441ef09fd035; _qimei_q32=06cf930e956c8a7ee7f1febc85e3dc8e; _qimei_q36=b46e45efc8afb74fcfa9f02130001fc19419; omgid=0_ccQAAksry1Ftm; yyb_muid=3A5F65E1CB6B64123F867075CA286545; _qpsvr_localtk=0.7918347375085358; ptui_loginuin=2636626273@qq.com; rewardsn=; wxtokenkey=777; ua_id=xHtW305mySbToIGDAAAAAC7dd7XVTFRXoaetlUE_nfM=; _clck=h9qu6k|1|fy2|0; wxuin=53933276328488; mm_lang=zh_CN; uuid=d3b84645ce92c0aaa9c1d7765b51d2de; rand_info=CAESIPjx85fQSe2l+XTkyaI/Ml9zaURJuYhBlBpJ3OJ6KyY/; slave_bizuin=3861526152; data_bizuin=3861526152; bizuin=3861526152; data_ticket=j01VKpjvcsaERu728fHlP7ktRYTABlGcnTOrS/GIN9lBclfN7+dQAZwuVnHpmA4Y; slave_sid=X1ZKY0JUY1g2UW01S285YWQwZjVoRnV6OWVYRklFX1JSalFGWXhBN1pNNjhLRDN0NGExV3JuR05WRzBOeWZFckFKMjRzeENXbF9mZUkyOEZJYzRjOWxCVG1ER01Oa0NkNF9WOG95Wl9Xc3FYcWt2MEdaQ1BLZE42OHUwY2ljRWcxdjNvV2RXZnNOcVFiOTk3; slave_user=gh_9645c6c4cd12; xid=d36806718995eba11241783870b95cf0; _clsk=ehna6n|1753941766992|7|1|mp.weixin.qq.com/weheat-agent/payload/record"

        # ç”¨æˆ·çš„User-Agentåˆ—è¡¨
        self.USER_AGENT_LIST = [
            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36 115Browser/6.0.3',
            'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',
            'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',
            'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11',
            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0',
            'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',
            "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.75 Mobile Safari/537.36",
        ]

        # åˆ›å»ºæ•°æ®æ–‡ä»¶å¤¹ç»“æ„
        os.makedirs("./data", exist_ok=True)
        for account_name in self.FAKEIDS.keys():
            os.makedirs(f"./data/{account_name}", exist_ok=True)
        os.makedirs("./data/mixed", exist_ok=True)

        # å­˜å‚¨æ–‡ç« æ•°æ®
        self.articles = []
        self.driver = None

    def get_headers(self):
        user_agent = random.choice(self.USER_AGENT_LIST)
        headers = {
            "Cookie": self.COOKIE,
            "User-Agent": user_agent,
        }
        return headers

    def get_fakeid(self,nickname):
        """è·å–å¾®ä¿¡å…¬ä¼—å·çš„ fakeid"""
        search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"

        params = {
            "action": "search_biz",
            "query": nickname,
            "begin": 0,
            "count": 1,
            "ajax": "1",
            "lang": "zh_CN",
            "f": "json",
            "token": self.TOKEN,
        }

        try:
            response = requests.get(search_url, headers=self.get_headers(), params=params)
            response.raise_for_status()
            data = response.json()

            if "list" in data and data["list"]:
                print(f"è·å–åˆ° {nickname} çš„ fakeid: {data['list'][0]['fakeid']}")
                return data["list"][0].get('fakeid')
            return None
        except requests.exceptions.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"è§£æå…¬ä¼—å· {nickname} çš„ fakeid å¤±è´¥")
            return None


    def fetch_article_hyperlinks(self, begin=0, count=5, account_name=None):
        """è·å–æ–‡ç« é“¾æ¥åˆ—è¡¨ - ä½¿ç”¨ç”¨æˆ·ä¿®æ­£åçš„APIå’Œå‚æ•°"""
        # ç¡®å®šä½¿ç”¨å“ªä¸ªå…¬ä¼—å·
        if account_name and account_name in self.FAKEIDS:
            target_fakeid = self.FAKEIDS[account_name]
            target_name = account_name
        else:
            target_name = list(self.FAKEIDS.keys())[0]
            target_fakeid = self.FAKEIDS[target_name]

        # ä½¿ç”¨ç”¨æˆ·ä¿®æ­£åçš„URLå’Œå‚æ•°ç»“æ„
        url = "https://mp.weixin.qq.com/cgi-bin/appmsgpublish"

        data = {
            "sub": 'list',
            "query": '',
            "begin": begin,
            "count": count,  # æ¯é¡µå°è¯•è·å–æœ€å¤š20ç¯‡
            "type": "101_1",
            "free_publish_type": "1",
            "search_field": None,
            "sub_action": 'list_ex',
            "fakeid": target_fakeid,
            "lang": "zh_CN",
            "f": "json",
            "ajax": 1,
            "token": self.TOKEN,
        }

        # éšæœºé€‰æ‹©User-Agent
        user_agent = random.choice(self.USER_AGENT_LIST)
        headers = {
            "Cookie": self.COOKIE,
            "User-Agent": user_agent,
        }

        try:
            print(f"ğŸ“¡ è¯·æ±‚æ–‡ç« é“¾æ¥: {target_name}")
            response = requests.get(url, headers=headers, params=data)

            if response.status_code != 200:
                print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []

            content_json = response.json()
            print(f"response: {content_json}")
            # æ£€æŸ¥è¿”å›ç»“æœ
            if "publish_page" not in content_json:
                print(f"è¿”å›æ•°æ®å¼‚å¸¸: {content_json}")
                return []

            results = []
            # è§£æå¤–å±‚JSON
            publish_page_str = content_json.get('publish_page', '{}')
            publish_page = json.loads(publish_page_str)

            # è·å–æ–‡ç« åˆ—è¡¨
            publish_list = publish_page.get('publish_list', [])

            for item in publish_list:
                publish_info_str = item.get('publish_info', '{}')

                publish_info = json.loads(publish_info_str)

                # è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯
                appmsgex = publish_info.get('appmsgex', [])

                for article in appmsgex:
                    # æå–æ‰€éœ€å­—æ®µ
                    title = article.get('title', '')
                    link = article.get('link', '')
                    author_name = article.get('author_name', '')

                    # è·å–å‘å¸ƒæ—¶é—´ï¼ˆä¼˜å…ˆä½¿ç”¨publish_infoä¸­çš„create_timeï¼‰
                    temp_time = publish_info.get('publish_info', {}).get('create_time', 0)
                    t = time.localtime(temp_time)
                    pub_time = time.strftime("%Y-%m-%d", t)
                    # æ„å»ºæ–‡ç« å­—å…¸
                    article_dict = {
                        "title": title,
                        "url": link,
                        "pub_time": pub_time,
                        "account_name": author_name
                    }

                    results.append(article_dict)
            return results

        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []

    def fetch_article_links(self, begin=0, count=5, account_name=None):
        """è·å–æ–‡ç« é“¾æ¥åˆ—è¡¨ - ä½¿ç”¨ç”¨æˆ·ä¿®æ­£åçš„APIå’Œå‚æ•°"""
        # ç¡®å®šä½¿ç”¨å“ªä¸ªå…¬ä¼—å·
        if account_name and account_name in self.FAKEIDS:
            target_fakeid = self.FAKEIDS[account_name]
            target_name = account_name
        else:
            target_name = list(self.FAKEIDS.keys())[0]
            target_fakeid = self.FAKEIDS[target_name]

        # ä½¿ç”¨ç”¨æˆ·ä¿®æ­£åçš„URLå’Œå‚æ•°ç»“æ„
        url = "https://mp.weixin.qq.com/cgi-bin/appmsg"

        data = {
            "token": self.TOKEN,
            "lang": "zh_CN",
            "f": "json",
            "ajax": "1",
            "action": "list_ex",
            "begin": str(begin),
            "count": str(count),
            "query": "",
            "fakeid": target_fakeid,
            "type": "9",
        }

        # éšæœºé€‰æ‹©User-Agent
        user_agent = random.choice(self.USER_AGENT_LIST)
        headers = {
            "Cookie": self.COOKIE,
            "User-Agent": user_agent,
        }

        try:
            print(f"ğŸ“¡ è¯·æ±‚æ–‡ç« é“¾æ¥: {target_name}")
            response = requests.get(url, headers=headers, params=data)

            if response.status_code != 200:
                print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []

            content_json = response.json()
            print(f"response: {content_json}")

            # æ£€æŸ¥è¿”å›ç»“æœ
            if "app_msg_list" not in content_json:
                print(f"è¿”å›æ•°æ®å¼‚å¸¸: {content_json}")
                return []

            results = []
            for item in content_json["app_msg_list"]:
                print("item:", item)
                title = item.get("title", "")
                link = item.get("link", "")
                create_time = item.get("create_time", 0)

                # è½¬æ¢æ—¶é—´æ ¼å¼
                t = time.localtime(create_time)
                pub_time = time.strftime("%Y-%m-%d", t)

                results.append({
                    "title": title,
                    "url": link,
                    "pub_time": pub_time,
                    "account_name": target_name
                })

            print(f"âœ… è·å–åˆ° {len(results)} ç¯‡æ–‡ç« é“¾æ¥")
            return results

        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []

    def fetch_article_content_requests(self, article):
        """ä½¿ç”¨requestsè·å–æ–‡ç« å†…å®¹"""
        try:
            print(f"ğŸ“„ æŠ“å–å†…å®¹: {article['title']}")

            # ä½¿ç”¨éšæœºUser-Agent
            user_agent = random.choice(self.USER_AGENT_LIST)
            headers = {"User-Agent": user_agent}

            res = requests.get(article["url"], headers=headers, timeout=15)

            if res.status_code != 200:
                print(f"âš ï¸ å†…å®¹è¯·æ±‚å¤±è´¥: {res.status_code}")
                return None

            soup = BeautifulSoup(res.text, "html.parser")

            # è·å–æ–‡ç« å†…å®¹
            content_elem = soup.select_one("div.rich_media_content")
            if not content_elem:
                content_elem = soup.select_one("#js_content")
            content = content_elem.get_text(separator="\n", strip=True) if content_elem else ""

            # è·å–æ–‡ç« æ ‡é¢˜
            title_elem = soup.select_one("h1.rich_media_title")
            title = title_elem.get_text(strip=True) if title_elem else article["title"]

            # è·å–ä½œè€…ä¿¡æ¯
            author_elem = soup.select_one("span.rich_media_meta_text")
            author = author_elem.get_text(strip=True) if author_elem else article.get("account_name", "æœªçŸ¥å…¬ä¼—å·")

            # è·å–å›¾ç‰‡é“¾æ¥
            images = []
            if content_elem:
                img_elements = content_elem.find_all('img')
                for img in img_elements:
                    src = img.get('data-src') or img.get('src')
                    if src and src.startswith('http'):
                        images.append(src)

            return {
                "title": title,
                "author": author,
                "account_name": article.get("account_name", "æœªçŸ¥å…¬ä¼—å·"),
                "pub_time": article["pub_time"],
                "url": article["url"],
                "content": content,
                "images": images,
                "word_count": len(content),
                "crawl_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

        except Exception as e:
            print(f"âŒ è·å–å†…å®¹å¤±è´¥: {e}")
            return None

    def setup_selenium_driver(self):
        """è®¾ç½®Selenium WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--lang=zh-CN")


            if os.path.exists(self.chrome_driver_path):
                print(f"ä½¿ç”¨æœ¬åœ°ChromeDriver: {self.chrome_driver_path}")
                service = Service(executable_path=self.chrome_driver_path)
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            else:
                print("æœ¬åœ°ChromeDriverä¸å­˜åœ¨ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½...")
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)

            print("âœ… Selenium WebDriver åˆå§‹åŒ–æˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ Selenium WebDriver åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def fetch_article_content_selenium(self, article):
        """ä½¿ç”¨Seleniumè·å–æ–‡ç« å†…å®¹"""
        try:
            if not self.driver:
                if not self.setup_selenium_driver():
                    return None

            self.driver.get(article["url"])
            time.sleep(2)  # é˜²æ­¢åŠ è½½ä¸å®Œå…¨

            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            div = soup.find("div", class_="rich_media_content")
            content = div.get_text(separator="\n", strip=True) if div else ""

            # è·å–æ ‡é¢˜
            title_elem = soup.select_one("h1.rich_media_title")
            title = title_elem.get_text(strip=True) if title_elem else article["title"]

            # è·å–ä½œè€…
            author_elem = soup.select_one("span.rich_media_meta_text")
            author = author_elem.get_text(strip=True) if author_elem else article.get("account_name", "æœªçŸ¥å…¬ä¼—å·")

            # è·å–å›¾ç‰‡
            images = []
            if div:
                img_elements = div.find_all('img')
                for img in img_elements:
                    src = img.get('data-src') or img.get('src')
                    if src and src.startswith('http'):
                        images.append(src)

            return {
                "title": title,
                "author": author,
                "account_name": article.get("account_name", "æœªçŸ¥å…¬ä¼—å·"),
                "pub_time": article["pub_time"],
                "url": article["url"],
                "content": content,
                "images": images,
                "word_count": len(content),
                "crawl_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

        except Exception as e:
            print(f"âŒ Seleniumè·å–å†…å®¹å¤±è´¥: {e}")
            return None

    def crawl_latest_article(self, use_selenium=False, account_name=None):
        """çˆ¬å–æœ€æ–°çš„ä¸€ç¯‡æ–‡ç« """
        target_account = account_name or list(self.FAKEIDS.keys())[0]
        print(f"ğŸš€ å¼€å§‹çˆ¬å– {target_account} æœ€æ–°æ–‡ç« ...")

        # è·å–æ–‡ç« é“¾æ¥
        links = self.fetch_article_links(begin=0, count=1, account_name=target_account)

        if not links:
            print("âŒ æœªè·å–åˆ°æ–‡ç« é“¾æ¥")
            return None

        latest_article = links[0]
        print(f"ğŸ“– æ‰¾åˆ°æœ€æ–°æ–‡ç« : {latest_article['title']}")

        # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«å°
        time.sleep(random.randint(2, 5))

        # è·å–æ–‡ç« è¯¦ç»†å†…å®¹
        if use_selenium:
            detail = self.fetch_article_content_selenium(latest_article)
            if detail is None:
                print("âš ï¸ Seleniumæ–¹æ³•å¤±è´¥ï¼Œå°è¯•requestsæ–¹æ³•...")
                detail = self.fetch_article_content_requests(latest_article)
        else:
            detail = self.fetch_article_content_requests(latest_article)

        if detail:
            self.articles = [detail]
            print(f"âœ… æˆåŠŸçˆ¬å–æ–‡ç« : {detail['title']}")
            print(f"ğŸ“± å…¬ä¼—å·: {detail['account_name']}")
            print(f"ğŸ“ æ–‡ç« å­—æ•°: {detail['word_count']} å­—")

            # ä¿å­˜æ–‡ä»¶
            self.save_to_json(account_name=target_account)
            self.save_to_excel(account_name=target_account)

            return detail
        else:
            print("âŒ æ–‡ç« å†…å®¹è·å–å¤±è´¥")
            return None

    def crawl_recent_articles(self, days_back=30, max_articles=10, use_selenium=False, account_name=None,auto_save=False):
        """çˆ¬å–æœ€è¿‘ä¸€æ®µæ—¶é—´çš„æ–‡ç« """
        target_account = account_name or list(self.FAKEIDS.keys())[0]
        print(f"ğŸš€ å¼€å§‹çˆ¬å– {target_account} æœ€è¿‘{days_back}å¤©çš„æ–‡ç« ï¼Œæœ€å¤š{max_articles}ç¯‡...")

        all_articles = []
        now = datetime.datetime.now()
        cutoff_date = now - datetime.timedelta(days=days_back)

        page = 0
        total_fetched = 0

        while total_fetched < max_articles:
            print(f"ğŸ“‹ æ­£åœ¨è·å–ç¬¬{page+1}é¡µæ–‡ç« é“¾æ¥...")
            links = self.fetch_article_links(begin=page*5, count=5, account_name=target_account)

            if not links:
                print("ğŸ“„ æ²¡æœ‰æ›´å¤šæ–‡ç« äº†")
                break

            for article in links:
                if total_fetched >= max_articles:
                    break

                temp_time = article["pub_time"]
                temp_title = article["title"]
                temp_url = f"./data/{account_name}/{temp_time}/{temp_title}.json"
                print(f"ğŸ”— å¤„ç†æ–‡ç« : {temp_url})")
                if os.path.exists(temp_url):
                    print(f"âš ï¸ æ–‡ç«  '{temp_title}' å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    continue

                try:
                    # æ£€æŸ¥æ–‡ç« æ—¥æœŸ
                    pub_dt = datetime.datetime.strptime(article["pub_time"], "%Y-%m-%d")
                    if pub_dt < cutoff_date:
                        print(f"â° æ–‡ç«  '{article['title']}' è¶…å‡ºæ—¶é—´èŒƒå›´ï¼Œè·³è¿‡")
                        break

                    # è·å–æ–‡ç« è¯¦ç»†å†…å®¹
                    if use_selenium:
                        detail = self.fetch_article_content_selenium(article)
                        if detail is None:
                            detail = self.fetch_article_content_requests(article)
                    else:
                        detail = self.fetch_article_content_requests(article)

                    if detail:
                        all_articles.append(detail)
                        total_fetched += 1
                        print(f"âœ… å·²å®Œæˆ {total_fetched}/{max_articles}")

                        if auto_save:
                            # ä¿å­˜å•ä¸ªæ–‡ç« åˆ°JSONæ–‡ä»¶
                            self.save_article_to_json(account_name=target_account,article=detail)

                        # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                        time.sleep(random.randint(15, 25))
                    else:
                        print(f"âŒ æ–‡ç« å†…å®¹è·å–å¤±è´¥: {article['title']}")

                except Exception as e:
                    print(f"âŒ å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                    continue

            page += 1
            time.sleep(random.randint(2, 5))  # é¡µé¢é—´å»¶è¿Ÿ

        self.articles = all_articles
        print(f"ğŸ‰ å…±æˆåŠŸçˆ¬å– {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles


    def crawl_recent_articles_to_md(self, days_back=30, max_articles=10,save_hook:callable=None, use_selenium=False, account_name=None,auto_save=False,auto_stop=False):
        """çˆ¬å–æœ€è¿‘ä¸€æ®µæ—¶é—´çš„æ–‡ç« """
        target_account = account_name or list(self.FAKEIDS.keys())[0]
        print(f"ğŸš€ å¼€å§‹çˆ¬å– {target_account} æœ€è¿‘{days_back}å¤©çš„æ–‡ç« ï¼Œæœ€å¤š{max_articles}ç¯‡...")

        all_articles = []
        now = datetime.datetime.now()
        cutoff_date = now - datetime.timedelta(days=days_back)

        page = 0
        total_fetched = 0
        failed_times = 0

        while total_fetched < max_articles:

            if auto_stop:
                if failed_times >= 3:
                    print("âŒ å·²æ— æœ€æ–°æ–‡ç« å¯çˆ¬å–")
                    break

            print(f"ğŸ“‹ æ­£åœ¨è·å–ç¬¬{page+1}é¡µæ–‡ç« é“¾æ¥...")
            links = self.fetch_article_hyperlinks(begin=page*5, count=5, account_name=target_account)

            if not links:
                print("ğŸ“„ æ²¡æœ‰æ›´å¤šæ–‡ç« äº†")
                break

            for article in links:
                if total_fetched >= max_articles:
                    break

                temp_time = article["pub_time"]
                temp_title = article["title"]
                temp_url = f"./data/{account_name}/{temp_time}/{temp_title}.md"
                print(f"ğŸ”— å¤„ç†æ–‡ç« : {temp_url})")
                if os.path.exists(temp_url):
                    print(f"âš ï¸ æ–‡ç«  '{temp_title}' å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    failed_times += 1
                    continue

                try:
                    # æ£€æŸ¥æ–‡ç« æ—¥æœŸ
                    pub_dt = datetime.datetime.strptime(article["pub_time"], "%Y-%m-%d")
                    date_str = pub_dt.strftime("%Y-%m-%d")
                    if pub_dt < cutoff_date:
                        print(f"â° æ–‡ç«  '{article['title']}' è¶…å‡ºæ—¶é—´èŒƒå›´ï¼Œè·³è¿‡")
                        break

                    if check_url_valid(article["url"]):
                        detail = extract_wechat_article_to_markdown(article["url"], date_str)
                        all_articles.append(detail)
                        total_fetched += 1
                        print(f"âœ… å·²å®Œæˆ {total_fetched}/{max_articles}")

                        if auto_save:
                            # ä¿å­˜å•ä¸ªæ–‡ç« åˆ°JSONæ–‡ä»¶
                            save_path = self.save_article_to_md(account_name=target_account,time = date_str,title=temp_title,article=detail)
                        if save_hook and save_path is not None:
                            save_hook(title = temp_title+".md",content = detail)

                        # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                        time.sleep(random.randint(15, 25))
                    else:
                        print(f"âŒ æ–‡ç« å†…å®¹è·å–å¤±è´¥: {article['title']}")

                except Exception as e:
                    print(f"âŒ å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                    failed_times += 1
                    continue

            page += 1
            time.sleep(random.randint(2, 5))  # é¡µé¢é—´å»¶è¿Ÿ

        self.articles = all_articles
        print(f"ğŸ‰ å…±æˆåŠŸçˆ¬å– {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles

    def crawl_all_accounts_latest(self, use_selenium=False):
        """éå†æ‰€æœ‰å…¬ä¼—å·ï¼Œçˆ¬å–æ¯ä¸ªå…¬ä¼—å·çš„æœ€æ–°æ–‡ç« """
        print(f"ğŸš€ å¼€å§‹éå†æ‰€æœ‰å…¬ä¼—å·çˆ¬å–æœ€æ–°æ–‡ç« ...")
        print(f"ğŸ“‹ å°†çˆ¬å–çš„å…¬ä¼—å·: {list(self.FAKEIDS.keys())}")

        all_articles = []

        for account_name in self.FAKEIDS.keys():
            print(f"\n{'='*50}")
            print(f"ğŸ“± æ­£åœ¨å¤„ç†å…¬ä¼—å·: {account_name}")

            try:
                latest = self.crawl_latest_article(use_selenium=use_selenium, account_name=account_name)
                if latest:
                    all_articles.append(latest)
                    print(f"âœ… {account_name} æœ€æ–°æ–‡ç« è·å–æˆåŠŸ")
                else:
                    print(f"âŒ {account_name} æœ€æ–°æ–‡ç« è·å–å¤±è´¥")

                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°ï¼ˆä½¿ç”¨ç”¨æˆ·åŸå§‹ä»£ç ä¸­çš„å»¶è¿Ÿç­–ç•¥ï¼‰
                print("â³ ç­‰å¾…åå¤„ç†ä¸‹ä¸€ä¸ªå…¬ä¼—å·...")
                time.sleep(random.randint(15, 25))

            except Exception as e:
                print(f"âŒ å¤„ç†å…¬ä¼—å· {account_name} æ—¶å‡ºé”™: {e}")
                continue

        # ä¿å­˜æ··åˆæ•°æ®
        if all_articles:
            self.articles = all_articles
            print(f"\nğŸ‰ éå†å®Œæˆï¼å…±æˆåŠŸçˆ¬å– {len(all_articles)} ä¸ªå…¬ä¼—å·çš„æœ€æ–°æ–‡ç« ")
            self.save_to_json()  # ä¿å­˜åˆ°mixedæ–‡ä»¶å¤¹
            self.save_to_excel()

        return all_articles

    def generate_filename(self, base_name, extension, account_name=None):
        """ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

        if account_name and account_name in self.FAKEIDS:
            folder = f"./data/{account_name}"
            filename = f"{base_name}_{timestamp}.{extension}"
        else:
            folder = "./data/mixed"
            filename = f"{base_name}_{timestamp}.{extension}"

        return os.path.join(folder, filename)

    def save_to_json(self, account_name=None):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not self.articles:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        base_name = "latest_article" if len(self.articles) == 1 else "recent_articles"
        filename = self.generate_filename(base_name, "json", account_name)

        os.makedirs(os.path.dirname(filename), exist_ok=True)

        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(self.articles, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ JSONæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
            return None

    def save_article_to_json(self, account_name, article):
        """ä¿å­˜articleæ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not article:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        folder = f"./data/{account_name}/"
        os.makedirs(os.path.dirname(folder), exist_ok=True)
        pubtime = article.get("pub_time")
        save_floder = folder + pubtime + "/"
        os.makedirs(os.path.dirname(save_floder), exist_ok=True)
        filename = save_floder + article.get('title',
                                             'NoneName' + datetime.datetime.now().strftime("%Y%m%d_%H%M%S")) + ".json"
        if os.path.exists(filename):
            print(f"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨: {filename}ï¼Œè·³è¿‡ä¿å­˜")
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(article, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ JSONæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
            return None

    def save_article_to_md(self, account_name, time, title, article):
        """ä¿å­˜articleæ•°æ®åˆ°mdæ–‡ä»¶"""
        if not article:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None

        # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        def clean_filename(filename):
            # å®šä¹‰Windows/Linux/Unixç³»ç»Ÿä¸­ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦
            illegal_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*', '!']
            for char in illegal_chars:
                filename = filename.replace(char, '_')
            # åŒæ—¶å»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹å·
            filename = filename.strip().strip('.')
            # é™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼Œé¿å…è·¯å¾„è¿‡é•¿
            if len(filename) > 100:
                filename = filename[:100]
            return filename

        folder = f"./data/{account_name}/"
        os.makedirs(os.path.dirname(folder), exist_ok=True)

        pubtime = time
        save_folder = folder + str(pubtime) + "/"
        os.makedirs(os.path.dirname(save_folder), exist_ok=True)

        # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
        clean_title = clean_filename(title)
        filename = save_folder + clean_title + ".md"

        if os.path.exists(filename):
            print(f"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨: {filename}ï¼Œè·³è¿‡ä¿å­˜")
            return filename

        try:
            with open(filename, "w", encoding="utf-8") as f:
                f.write(article)
            print(f"ğŸ’¾ mdæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜mdæ–‡ä»¶å¤±è´¥: {e}")
            return None



    def save_to_json_single_File(self, account_name=None):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not self.articles:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        folder = f"./data/{account_name}/"
        os.makedirs(os.path.dirname(folder), exist_ok=True)
        for article in self.articles:
            pubtime = article.get("pub_time")
            save_floder = folder + pubtime + "/"
            os.makedirs(os.path.dirname(save_floder), exist_ok=True)
            filename = save_floder + article.get('title', 'NoneName'+ datetime.datetime.now().strftime("%Y%m%d_%H%M%S")) + ".json"
            if os.path.exists(filename):
                print(f"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨: {filename}ï¼Œè·³è¿‡ä¿å­˜")
                continue
            try:
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(article, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ JSONæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
                return filename
            except Exception as e:
                print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
                return None


    def save_to_excel(self, account_name=None):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        if not self.articles:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        base_name = "latest_article" if len(self.articles) == 1 else "recent_articles"
        filename = self.generate_filename(base_name, "xlsx", account_name)

        os.makedirs(os.path.dirname(filename), exist_ok=True)

        try:
            df_data = []
            for article in self.articles:
                df_data.append({
                    'å…¬ä¼—å·': article.get('account_name', 'æœªçŸ¥å…¬ä¼—å·'),
                    'æ ‡é¢˜': article['title'],
                    'ä½œè€…': article['author'],
                    'å‘å¸ƒæ—¶é—´': article['pub_time'],
                    'æ–‡ç« é“¾æ¥': article['url'],
                    'å­—æ•°': article['word_count'],
                    'å›¾ç‰‡æ•°é‡': len(article['images']),
                    #'å†…å®¹æ‘˜è¦': article['content'][:200] + '...' if len(article['content']) > 200 else article['content'],
                    'å†…å®¹æ‘˜è¦': article['content'],
                    'çˆ¬å–æ—¶é—´': article['crawl_time']
                })

            df = pd.DataFrame(df_data)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"ğŸ“Š Excelæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
            return None

    def save_to_csv(self, account_name=None):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆå…¼å®¹ç”¨æˆ·åŸå§‹ä»£ç çš„ä¿å­˜æ ¼å¼ï¼‰"""
        if not self.articles:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        base_name = "articles_list"
        filename = self.generate_filename(base_name, "csv", account_name)

        os.makedirs(os.path.dirname(filename), exist_ok=True)

        try:
            df_data = []
            for article in self.articles:
                df_data.append({
                    'title': article['title'],
                    'link': article['url'],
                    'create_time': article['pub_time']
                })

            df = pd.DataFrame(df_data)
            df.to_csv(filename, mode='a', encoding='utf-8', index=False)
            print(f"ğŸ“„ CSVæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
            return None

    def close_driver(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            try:
                self.driver.quit()
                print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
            except:
                pass

def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    spider = WeChatSpider()

    try:
        print("ğŸ¯ åŸºäºç”¨æˆ·ä¿®æ­£ä»£ç çš„å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«")
        print(f"ğŸ“‹ æ”¯æŒçš„å…¬ä¼—å·: {list(spider.FAKEIDS.keys())}")

        # æµ‹è¯•å•ä¸ªå…¬ä¼—å·
        '''
        print("\n" + "="*50)
        print("æµ‹è¯•: çˆ¬å–å¨ç§‘å…ˆè¡Œæœ€æ–°æ–‡ç« ")
        print("="*50)
        
        latest = spider.crawl_latest_article(use_selenium=False, account_name="å¨ç§‘å…ˆè¡Œ")
        if latest:
            print(f"âœ… æµ‹è¯•æˆåŠŸï¼")
            print(f"ğŸ“– æ ‡é¢˜: {latest['title']}")
            print(f"ğŸ“ å­—æ•°: {latest['word_count']}")
        '''

        # æµ‹è¯•æ‰€æœ‰å…¬ä¼—å·
        spider.articles = []  # æ¸…ç©ºæ•°æ®

        print("\n" + "="*50)
        print("æµ‹è¯•: çˆ¬å–æ‰€æœ‰å…¬ä¼—å·æœ€æ–°æ–‡ç« ")
        print("="*50)

        all_latest = spider.crawl_all_accounts_latest(use_selenium=False)
        if all_latest:
            print(f"âœ… æ‰€æœ‰å…¬ä¼—å·æµ‹è¯•æˆåŠŸï¼Œå…±è·å– {len(all_latest)} ç¯‡æ–‡ç« ")
            for article in all_latest:
                print(f"ğŸ“± {article['account_name']}: {article['title']} ({article['word_count']} å­—)")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    finally:
        spider.close_driver()

if __name__ == "__main__":
    main()